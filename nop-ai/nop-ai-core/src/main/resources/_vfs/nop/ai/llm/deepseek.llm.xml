<llm x:schema="/nop/schema/ai/llm.xdef" xmlns:x="/nop/schema/xdsl.xdef"
     apiStyle="openai" defaultModel="deepseek-chat">

    <baseUrl>https://api.deepseek.com</baseUrl>
    <chatUrl>/chat/completions</chatUrl>
    <generateUrl>/api/generate</generateUrl>
    <embedUrl>/api/embed</embedUrl>

    <models>
        <model name="qwen3" disableThinkingPrompt="/no_think"/>
    </models>

    <request seedPath="options.seed"
             topPPath="top_p"
             temperaturePath="temperature"
             stopPath="stop"
             maxTokensPath="max_tokens"
    />

    <response contentPath="choices.0.message.content"
              rolePath="choices.0.message.role"
              promptTokensPath="usage.prompt_tokens"
              completionTokensPath="usage.completion_tokens"
              totalTokensPath="usage.total_tokens"
              reasoningContentPath="choices.0.message.reasoning_content"
              statusPath="done"
              errorPath="error"
    />

</llm>